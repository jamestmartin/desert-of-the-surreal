# Fractional and complex truth values
my current belief is that syntactically, "number of truth values" corresponds with "number of canonical forms". Each integer designation corresponds with a different normal form.

I don't believe "0" is the correct designation for "is false"; I think it just stands for "no evidence". In most proof assistants there is no false designation per se; you can only assign true (1) to a proposition, assign true (1) to its negation, and have a non-contradiction axiom which effectively defines falsity (-1) as the assignment of true to the negative. a system which allows assigning falsehood ought to treat A and -A as the same proposition. I believe this is the correct interpretation of negative semantic values.

I believe fractional designations correspond with partial evidence. Total programming languages only have non-zero integer truth values (terms which are wholly defined), but partial languages such as Haskell have terms which are not fully defined. I think undefined :: a is the designation of "0" to any proposition. Designations between 0 and 1 perhaps correspond with computational complexity; 1 is fully determined and evaluated; 0 is fully undetermined and non-terminating; values in-between are complex computations, where polynomial time/space is closer to 1 and exponential/etc is closer to 0, in the sense of practicality of getting a complete truth value. I believe the quantum analog of computational complexity is accumulated error, which I think is more intuitive as a truth value. Induction is partial in the sense that you accumulate error with each iteration, which gives you fractional truth.

More complex Hasse diagram-style domains are the result of combining fractional truth values with integer truth values, generating a partial order of truth values between 0 and various other integer values instead of just a sliding scale between 0 and 1.

Integrating computational complexity, truth values need not be discrete. Integer truth values perhaps give you some sort of plane or circle or something, I don't know because I'm not used to thinking in spatial terms and haven't thought it through. But the truth space need not be flat nor euclidian, either. Two syntactic "coordinates" (canonical terms) may coincide (be equal, according to higher-inductive types). However, unlike with complexity, the path between these points is orthogonal to the path between integer-like points, and the space in travelling the "wormhole" between these points is still what I believe to be "imaginary" designations, or entanglements, or something. I could still be wrong about this, but I am confident that these spatial conditions will become relevant in some way as we progress.

Which brings us to the interpretation of EM, NC, triviality, UIP, and univalence.

NC I believe is a statement about the disjointness of positive and negative designations. Basically, if a proposition is partially true (".75") (i.e. you have a partially-defined term), then the parts of that term which can be designated false ("-.25") are at most the parts of the term which are undefined. You can have NC in a partial system like Haskell because the application of NC between the portion of your term which is undefined and the portion of your term is false is itself undefined because that portion of the term is not at all true. So even in a system which is inconsistent in the classical sense (as I said, undefined :: a), non-contradiction still holds meaningfully if you interpret it in terms of partial truth values, because you never actually have a -1 and 1; the sum of the magnitudes is at most one, and the parts which are true and the parts which are false are disjoint. If you only have one truth value 1, then the disjointness condition is moot because the proposition can only be true in one way, so it's purely fractional; in a system with integer truth values, then you have to start worrying about disjointness of truth.

so if NC is generalized to something along the lines of "the sum of the magnitudes of opposite truth and false values is at most 1", then EM is the condition that the sum is at least 1. a system with EM but not NC could still be overdefined. EM is related to equational inconsistency in intuitionistic systems precisely because of the lack of negative designations. As I said earlier, you can only assign truth to the negative, not falsity per se. But the negative is trivial/non-factive (only has at most one inhabitant) in intuitionistic systems, so asserting the sum of the truth values is at least 1 forces all integer truth values to have the same magnitude (by transitivity and symmetry using EM).

univalence is then the definition of equivalence for when you have integer truth values. types with precisely the same quantity of truth values (up to a path) are equal (up to a path). however, univalence is going to be *false* (or at least trivial) in a system with fractional truth values, because of the computational complexity interpretation of the shapes of types which I described earlier.

UIP is some sort of flatness condition.... my train of thought is kinda petering off for now.

> I would like to hopefully clarify my meaning somewhat. The set {T, F} has two semantic values. |{T,F}| = 2. A set of 0 semantic values in classical terms is {} such that |{}| = 0. For a set to have a negative number of semantic values it can't just be {-1} because that set has 1 semantic value |{-1}| = 1. Similar for non-real numbers of semantic values. For a set to have a fractional number of semantic values {T, F^z} it would have to be the case that F has fuzzy or complex-fuzzy membership in the set and the set overall is not a crisp set such that |{T, F^z}| = 1 + |z|.
>
> This is distinctly different from a proposition having a negative semantic value in a system with 1 semantic value, {-1}: $$p\vdash^{-1} p$$
>
> [...]
>
> I'll also take the opportunity to point out that "the space travelling the 'wormhole' between these points is still what I believe to be 'imaginary' designations, or entanglements, or something." is actually the current accepted conjecture by myself and Zizzi; entanglements are thought to be some kind of wormhole in spacetime per something called https://en.wikipedia.org/wiki/ER_%3D_EPR.
>
> [...]
>
> This is part of why I conjectured that unitarity in quantum mechanics is basically a generalization or equivalent to univalence in semi-classical type theories; the unitarity of quantum mechanics is a significant part of why we use generalized Euclidean spaces (Hilbert spaces) to express the space of quantum mechanical systems. IE a flat space. 
